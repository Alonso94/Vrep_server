{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected\n",
      "\n",
      " Start training \n",
      "\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f9b602a92b0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f9b602a92b0>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f9b6015ff28>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f9b6015ff28>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0005640239 |\n",
      "| clipfrac           | 0.0          |\n",
      "| explained_variance | -0.00323     |\n",
      "| fps                | 0            |\n",
      "| n_updates          | 1            |\n",
      "| policy_entropy     | 8.513976     |\n",
      "| policy_loss        | -0.004296137 |\n",
      "| serial_timesteps   | 128          |\n",
      "| time_elapsed       | 5.25e-06     |\n",
      "| total_timesteps    | 128          |\n",
      "| value_loss         | 1087.3632    |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "from env.env_sim import rozum_sim\n",
    "env=rozum_sim()\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines.common.policies import FeedForwardPolicy\n",
    "# from stable_baselines import SAC\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import results_plotter\n",
    "\n",
    "\n",
    "# print(\"\\n Before training \\n\")\n",
    "# obs = env.reset()\n",
    "# for i in range(10):\n",
    "#     action = env.sample_action()\n",
    "#     obs, reward, done, info = env.step(action)\n",
    "#     print(reward)\n",
    "#     if done:\n",
    "#         env.reset()\n",
    "\n",
    "print(\"\\n Start training \\n\")    \n",
    "# env = gym.make('Pendulum-v0')\n",
    "\n",
    "class CustomPolicy(FeedForwardPolicy):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(CustomPolicy, self).__init__(*args, **kwargs,\n",
    "                                           net_arch=[dict(pi=[128, 128, 64,32], vf=[128, 128, 64,32])],\n",
    "                                           feature_extraction=\"mlp\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "        \n",
    "model = PPO2(CustomPolicy, env, verbose=1,tensorboard_log=\"PPO_rozum\")\n",
    "model.learn(total_timesteps=10000, log_interval=10,tb_log_name=\"test\")\n",
    "model.save(\"PPO_rozum\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = SAC.load(\"PPO_rozum\")\n",
    "print(\"\\n After training \\n\")\n",
    "obs = env.reset()\n",
    "for i in range(50):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        env.reset()\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env.env_sim import rozum_sim\n",
    "env=rozum_sim()\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "\n",
    "from stable_baselines.sac.policies import MlpPolicy\n",
    "from stable_baselines import SAC\n",
    "from stable_baselines import results_plotter\n",
    "\n",
    "os.chdir(\"/\")\n",
    "model = SAC.load(\"sac_rozum.zip\",env=env,tensorboard_log=\"sac_rozum_2\")\n",
    "# model.learn(total_timesteps=10000, log_interval=10,tb_log_name=\"stage2\")\n",
    "# model.save(\"sac_rozum2\")\n",
    "print(\"\\n After training \\n\")\n",
    "obs = env.reset()\n",
    "for i in range(50):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    print(reward)\n",
    "    if done:\n",
    "        env.reset()\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
